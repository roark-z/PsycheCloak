# Created by Hao Hu 2023
# Converts csv files containing interviews generated by chatgpt api
# to json files useable to train on alpaca LLM

import csv
import json
import random
import numpy as np
import os
def read_CSV2List(csv_file_path):
    # read CSV file
    with open(csv_file_path, "r") as csv_file:
        reader = csv.DictReader(csv_file)
        data  = list(reader)
    output_L = []
    for i in range(len(data)):
        output_L.append(list(data[i].values())[0])
    return output_L
def encode_instance(instruction, input, output):
    prompt_template     = "instruction: {instruction}\ninput: {input}\nOutput:"
    completion_template = "{output}<|endoftext|>"
    prompt = prompt_template.format(instruction=instruction.strip(), input=input.strip())
    completion = completion_template.format(output=output.strip())
    data = {
        "prompt": prompt,
        "completion": completion,
        "instruction": instruction.strip(),
        "input": input.strip(),
        "output": output.strip(),
    }
    return data
def Prepare_fine_tuning_data(output_dir, input_path = None, input_batch = None, n_max = -1):
    assert((input_path is not None) or (input_batch is not None) )
    if  input_path is not None:
        instruction_file_path = input_path['instruction']
        output_file_path      = input_path['output']
        input_file_path       = input_path['input']
        instruction_batch = read_CSV2List(instruction_file_path)
        output_batch      = read_CSV2List(output_file_path)
        assert(len(instruction_batch) == len(output_batch))
        if  input_file_path is not None:
            input_batch   = read_CSV2List(input_file_path)
        else:
            input_batch   = [''] * len(instruction_batch)    
    else:
        instruction_batch = input_batch['instruction']
        output_batch      = input_batch['output']
        input_batch       = input_batch['input']
    # get the prompt and completion for training gpt3
    gpt3_instances = []
    if  n_max <= 0 :
        n_max = len(instruction_batch)
    else:
        n_max = min(len(instruction_batch), n_max)
        
    for i in range(n_max):        
        gpt3_instances.append(encode_instance(instruction_batch[i], input_batch[i], output_batch[i]))
    with open(os.path.join(output_dir, f"PsycheCloak_data_{len(gpt3_instances)}.json"), "w") as fout:
        json.dump(gpt3_instances, fout)

